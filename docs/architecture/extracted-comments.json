[
  {
    "file_path": "docker-compose.yml",
    "line_number": 48,
    "element_name": "vision-ai",
    "language": "yaml",
    "llm_type": "config",
    "llm_legend": "Docker service configuration for AI-powered image analysis microservice",
    "llm_key": "Defines Python container with BLIP model, Flask HTTP server, and persistent model storage",
    "llm_map": "Part of microservices architecture, connects backend to vision processing capabilities",
    "llm_axiom": "Vision service must be accessible on port 8001 for backend integration",
    "llm_contract": "Service must respond to health checks within 30 seconds and handle image uploads",
    "llm_token": "vision-models: Docker volume for persistent transformer model cache",
    "raw_comment": "@llm-type config\n@llm-legend Docker service configuration for AI-powered image analysis microservice\n@llm-key Defines Python container with BLIP model, Flask HTTP server, and persistent model storage\n@llm-map Part of microservices architecture, connects backend to vision processing capabilities\n@llm-axiom Vision service must be accessible on port 8001 for backend integration\n@llm-contract Service must respond to health checks within 30 seconds and handle image uploads\n@llm-token vision-models: Docker volume for persistent transformer model cache",
    "context": "vision-ai:"
  },
  {
    "file_path": "services/vision-ai/main.py",
    "line_number": 2,
    "element_name": "unknown",
    "language": "python",
    "llm_type": "service",
    "llm_legend": "Provides AI-powered image analysis using BLIP vision model for user-uploaded content",
    "llm_key": "Loads BLIP model on startup, serves Flask HTTP API on port 8001, implements health checks",
    "llm_map": "Entry point for vision processing pipeline, integrates with backend via HTTP API",
    "llm_axiom": "Vision model must be loaded and ready before accepting any processing requests",
    "llm_contract": "Returns structured analysis JSON or appropriate HTTP error codes for failures",
    "llm_token": "BLIP: Bootstrapping Language-Image Pre-training model for image captioning",
    "raw_comment": "@llm-type service\n@llm-legend Provides AI-powered image analysis using BLIP vision model for user-uploaded content\n@llm-key Loads BLIP model on startup, serves Flask HTTP API on port 8001, implements health checks\n@llm-map Entry point for vision processing pipeline, integrates with backend via HTTP API\n@llm-axiom Vision model must be loaded and ready before accepting any processing requests\n@llm-contract Returns structured analysis JSON or appropriate HTTP error codes for failures\n@llm-token BLIP: Bootstrapping Language-Image Pre-training model for image captioning\n\nVision AI Service - Main Entry Point\nStarts both Flask HTTP server and gRPC server based on environment variables",
    "context": ""
  },
  {
    "file_path": "services/vision-ai/main.py",
    "line_number": 26,
    "element_name": "start_flask_server",
    "language": "python",
    "llm_type": "function",
    "llm_legend": "Starts Flask HTTP server to handle image analysis requests from backend",
    "llm_key": "Binds to all interfaces on port 8001, disables debug mode for production",
    "llm_map": "Called by main thread, serves HTTP endpoints defined in app.py",
    "llm_axiom": null,
    "llm_contract": "Blocks until server shutdown, logs startup status",
    "llm_token": null,
    "raw_comment": "@llm-type function\n@llm-legend Starts Flask HTTP server to handle image analysis requests from backend\n@llm-key Binds to all interfaces on port 8001, disables debug mode for production\n@llm-map Called by main thread, serves HTTP endpoints defined in app.py\n@llm-contract Blocks until server shutdown, logs startup status",
    "context": "def start_flask_server():"
  },
  {
    "file_path": "frontend/src/services/AudioService.ts",
    "line_number": 41,
    "element_name": "AudioService",
    "language": "typescript",
    "llm_type": "service",
    "llm_legend": "Provides frontend audio processing capabilities including speech-to-text transcription",
    "llm_key": "Uses Fetch API with FormData for audio upload, implements error handling and retry logic",
    "llm_map": "Frontend service layer, communicates with whisper-tts service on port 8000",
    "llm_axiom": "All audio operations must provide user feedback and handle network failures gracefully",
    "llm_contract": "Returns structured responses or throws descriptive errors for UI handling",
    "llm_token": "whisper-service: Python microservice providing speech-to-text capabilities",
    "raw_comment": "@llm-type service\n@llm-legend Provides frontend audio processing capabilities including speech-to-text transcription\n@llm-key Uses Fetch API with FormData for audio upload, implements error handling and retry logic\n@llm-map Frontend service layer, communicates with whisper-tts service on port 8000\n@llm-axiom All audio operations must provide user feedback and handle network failures gracefully\n@llm-contract Returns structured responses or throws descriptive errors for UI handling\n@llm-token whisper-service: Python microservice providing speech-to-text capabilities\n\nAudio service for pure functional API communication\nFollowing the same pattern as ChatService.ts",
    "context": "export class AudioService {"
  },
  {
    "file_path": "frontend/src/services/AudioService.ts",
    "line_number": 60,
    "element_name": "formData",
    "language": "typescript",
    "llm_type": "function",
    "llm_legend": "Converts user audio recordings to text using AI speech recognition",
    "llm_key": "Creates FormData with audio blob, sends POST to whisper service, handles JSON response",
    "llm_map": "Core transcription function called by UI components, integrates with whisper-tts service",
    "llm_axiom": null,
    "llm_contract": "Accepts audio Blob, returns TranscriptionResponse with text and language, throws on errors",
    "llm_token": null,
    "raw_comment": "@llm-type function\n@llm-legend Converts user audio recordings to text using AI speech recognition\n@llm-key Creates FormData with audio blob, sends POST to whisper service, handles JSON response\n@llm-map Core transcription function called by UI components, integrates with whisper-tts service\n@llm-contract Accepts audio Blob, returns TranscriptionResponse with text and language, throws on errors\n\nPure function: Transcribe audio blob to text\nInput: Blob (audio data)\nOutput: Promise<TranscriptionResponse>\n\nUses the EXACT same approach as our working HTML implementation",
    "context": "const formData = new FormData();"
  },
  {
    "file_path": "scripts/docs/validate-llm-comments.py",
    "line_number": 2,
    "element_name": "unknown",
    "language": "python",
    "llm_type": null,
    "llm_legend": null,
    "llm_key": null,
    "llm_map": null,
    "llm_axiom": null,
    "llm_contract": null,
    "llm_token": null,
    "raw_comment": "LLM Comment Validation System\n\nValidates consistency and quality of @llm-* comments across the codebase.\nIntegrates with the existing documentation validation workflow.",
    "context": ""
  },
  {
    "file_path": "scripts/docs/extract-llm-comments.py",
    "line_number": 2,
    "element_name": "unknown",
    "language": "python",
    "llm_type": null,
    "llm_legend": null,
    "llm_key": null,
    "llm_map": null,
    "llm_axiom": null,
    "llm_contract": null,
    "llm_token": null,
    "raw_comment": "LLM Comment Extraction System\n\nParses @llm-* tags from code comments across all programming languages\nin the Unhinged monorepo and generates architectural documentation.",
    "context": ""
  },
  {
    "file_path": "scripts/docs/extract-llm-comments.py",
    "line_number": 131,
    "element_name": "_parse_llm_tags",
    "language": "python",
    "llm_type": null,
    "llm_legend": null,
    "llm_key": null,
    "llm_map": null,
    "llm_axiom": null,
    "llm_contract": null,
    "llm_token": null,
    "raw_comment": "Parse @llm-* tags from comment text",
    "context": "def _parse_llm_tags(self, comment_text: str, file_path: str, line_number: int,"
  },
  {
    "file_path": "scripts/docs/extract-llm-comments.py",
    "line_number": 241,
    "element_name": "_parse_llm_tags",
    "language": "python",
    "llm_type": null,
    "llm_legend": null,
    "llm_key": null,
    "llm_map": null,
    "llm_axiom": null,
    "llm_contract": null,
    "llm_token": null,
    "raw_comment": "Parse @llm-* tags from Python docstring",
    "context": "def _parse_llm_tags(self, comment_text: str, file_path: str, line_number: int,"
  },
  {
    "file_path": "scripts/docs/extract-llm-comments.py",
    "line_number": 483,
    "element_name": "generate_architectural_overview",
    "language": "python",
    "llm_type": null,
    "llm_legend": null,
    "llm_key": null,
    "llm_map": "comments",
    "llm_axiom": null,
    "llm_contract": null,
    "llm_token": null,
    "raw_comment": "Generate architectural overview from @llm-map comments",
    "context": "def generate_architectural_overview(self) -> str:"
  },
  {
    "file_path": "backend/src/main/kotlin/com/unhinged/infrastructure/vision/HttpVisionProcessingService.kt",
    "line_number": 68,
    "element_name": "HttpVisionProcessingService",
    "language": "kotlin",
    "llm_type": "service",
    "llm_legend": "Enables backend to request AI-powered image analysis from vision-ai microservice",
    "llm_key": "Uses Ktor HTTP client with JSON serialization, implements retry logic and error handling",
    "llm_map": "Infrastructure layer implementation, called by application services, connects to vision-ai on port 8001",
    "llm_axiom": "All HTTP calls must have timeouts and proper error handling to prevent system hangs",
    "llm_contract": "Returns VisionResult on success or throws VisionProcessingException on failures",
    "llm_token": "vision-ai-service: Python microservice running BLIP model for image analysis",
    "raw_comment": "@llm-type service\n@llm-legend Enables backend to request AI-powered image analysis from vision-ai microservice\n@llm-key Uses Ktor HTTP client with JSON serialization, implements retry logic and error handling\n@llm-map Infrastructure layer implementation, called by application services, connects to vision-ai on port 8001\n@llm-axiom All HTTP calls must have timeouts and proper error handling to prevent system hangs\n@llm-contract Returns VisionResult on success or throws VisionProcessingException on failures\n@llm-token vision-ai-service: Python microservice running BLIP model for image analysis\n\nHTTP client implementation of VisionProcessingService\n\nCommunicates with the Python vision-ai service via HTTP API",
    "context": "class HttpVisionProcessingService("
  }
]