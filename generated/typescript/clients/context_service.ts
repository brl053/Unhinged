// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v1.181.2
//   protoc               v3.21.12
// source: context_service.proto

/* eslint-disable */
import {
  type CallOptions,
  ChannelCredentials,
  Client,
  type ClientOptions,
  type ClientUnaryCall,
  type handleUnaryCall,
  makeGenericClientConstructor,
  Metadata,
  type ServiceError,
  type UntypedServiceImplementation,
} from "@grpc/grpc-js";
import Long from "long";
import _m0 from "protobufjs/minimal";
import { Empty, HealthResponse } from "./vision_service";

/** Request for prompt generation with context */
export interface PromptGenerationRequest {
  /** Base prompt to enhance */
  basePrompt: string;
  /** Type of analysis (screenshot, natural_image, document, ui_component) */
  analysisType: string;
  /** Types of context to include (documentation, ui_components, api_endpoints, architecture) */
  contextTypes: string[];
  /** Maximum number of context items to include */
  maxContextItems: number;
  /** Additional context hints */
  contextHints: { [key: string]: string };
  /** LLM model to use for generation */
  model: string;
  /** Maximum tokens for enhanced prompt */
  maxTokens: number;
  /** Temperature for generation */
  temperature: number;
}

export interface PromptGenerationRequest_ContextHintsEntry {
  key: string;
  value: string;
}

/** Response with enhanced prompt */
export interface PromptGenerationResponse {
  /** Enhanced prompt with context */
  enhancedPrompt: string;
  /** Number of context items used */
  contextItemsUsed: number;
  /** Context items that were included */
  contextItems: ContextItem[];
  /** Processing time in seconds */
  processingTime: number;
  /** Model used for generation */
  modelUsed: string;
  /** Success status */
  success: boolean;
  /** Error message if generation failed */
  error: string;
}

/** Request for context search */
export interface ContextSearchRequest {
  /** Search query */
  query: string;
  /** Types of context to search (documentation, ui_components, api_endpoints, architecture) */
  contextTypes: string[];
  /** Maximum number of results */
  maxResults: number;
  /** Minimum relevance score (0.0 to 1.0) */
  minRelevance: number;
  /** Additional search filters */
  filters: { [key: string]: string };
}

export interface ContextSearchRequest_FiltersEntry {
  key: string;
  value: string;
}

/** Response with context search results */
export interface ContextSearchResponse {
  /** Found context items */
  results: ContextItem[];
  /** Total number of results found */
  totalResults: number;
  /** Search query that was executed */
  query: string;
  /** Processing time in seconds */
  processingTime: number;
}

/** Request for text generation */
export interface TextGenerationRequest {
  /** Input prompt */
  prompt: string;
  /** LLM model to use */
  model: string;
  /** Maximum tokens to generate */
  maxTokens: number;
  /** Temperature for generation (0.0 to 1.0) */
  temperature: number;
  /** Top-p sampling parameter */
  topP: number;
  /** Stop sequences */
  stopSequences: string[];
  /** Additional generation parameters */
  parameters: { [key: string]: string };
}

export interface TextGenerationRequest_ParametersEntry {
  key: string;
  value: string;
}

/** Response with generated text */
export interface TextGenerationResponse {
  /** Generated text */
  text: string;
  /** Model used for generation */
  modelUsed: string;
  /** Number of tokens generated */
  tokensGenerated: number;
  /** Processing time in seconds */
  processingTime: number;
  /** Success status */
  success: boolean;
  /** Error message if generation failed */
  error: string;
}

/** Context item from documentation or codebase */
export interface ContextItem {
  /** Unique identifier */
  id: string;
  /** Type of context (documentation, ui_component, api_endpoint, architecture) */
  type: string;
  /** Title or name */
  title: string;
  /** Content or description */
  content: string;
  /** Source file path */
  filePath: string;
  /** Associated tags */
  tags: string[];
  /** Relevance score (0.0 to 1.0) */
  relevanceScore: number;
  /** Last modified timestamp */
  lastModified: string;
  /** Additional metadata */
  metadata: { [key: string]: string };
}

export interface ContextItem_MetadataEntry {
  key: string;
  value: string;
}

/** Response with available LLM models */
export interface LLMModelsResponse {
  models: LLMModelInfo[];
}

/** Information about an LLM model */
export interface LLMModelInfo {
  name: string;
  displayName: string;
  description: string;
  available: boolean;
  /** ollama, openai, anthropic */
  provider: string;
  maxTokens: number;
  /** text_generation, prompt_enhancement, etc. */
  capabilities: string[];
}

function createBasePromptGenerationRequest(): PromptGenerationRequest {
  return {
    basePrompt: "",
    analysisType: "",
    contextTypes: [],
    maxContextItems: 0,
    contextHints: {},
    model: "",
    maxTokens: 0,
    temperature: 0,
  };
}

export const PromptGenerationRequest = {
  encode(message: PromptGenerationRequest, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.basePrompt !== "") {
      writer.uint32(10).string(message.basePrompt);
    }
    if (message.analysisType !== "") {
      writer.uint32(18).string(message.analysisType);
    }
    for (const v of message.contextTypes) {
      writer.uint32(26).string(v!);
    }
    if (message.maxContextItems !== 0) {
      writer.uint32(32).int32(message.maxContextItems);
    }
    Object.entries(message.contextHints).forEach(([key, value]) => {
      PromptGenerationRequest_ContextHintsEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).ldelim();
    });
    if (message.model !== "") {
      writer.uint32(50).string(message.model);
    }
    if (message.maxTokens !== 0) {
      writer.uint32(56).int32(message.maxTokens);
    }
    if (message.temperature !== 0) {
      writer.uint32(69).float(message.temperature);
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): PromptGenerationRequest {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePromptGenerationRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.basePrompt = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.analysisType = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.contextTypes.push(reader.string());
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.maxContextItems = reader.int32();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = PromptGenerationRequest_ContextHintsEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.contextHints[entry5.key] = entry5.value;
          }
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.model = reader.string();
          continue;
        case 7:
          if (tag !== 56) {
            break;
          }

          message.maxTokens = reader.int32();
          continue;
        case 8:
          if (tag !== 69) {
            break;
          }

          message.temperature = reader.float();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PromptGenerationRequest {
    return {
      basePrompt: isSet(object.basePrompt) ? globalThis.String(object.basePrompt) : "",
      analysisType: isSet(object.analysisType) ? globalThis.String(object.analysisType) : "",
      contextTypes: globalThis.Array.isArray(object?.contextTypes)
        ? object.contextTypes.map((e: any) => globalThis.String(e))
        : [],
      maxContextItems: isSet(object.maxContextItems) ? globalThis.Number(object.maxContextItems) : 0,
      contextHints: isObject(object.contextHints)
        ? Object.entries(object.contextHints).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      maxTokens: isSet(object.maxTokens) ? globalThis.Number(object.maxTokens) : 0,
      temperature: isSet(object.temperature) ? globalThis.Number(object.temperature) : 0,
    };
  },

  toJSON(message: PromptGenerationRequest): unknown {
    const obj: any = {};
    if (message.basePrompt !== "") {
      obj.basePrompt = message.basePrompt;
    }
    if (message.analysisType !== "") {
      obj.analysisType = message.analysisType;
    }
    if (message.contextTypes?.length) {
      obj.contextTypes = message.contextTypes;
    }
    if (message.maxContextItems !== 0) {
      obj.maxContextItems = Math.round(message.maxContextItems);
    }
    if (message.contextHints) {
      const entries = Object.entries(message.contextHints);
      if (entries.length > 0) {
        obj.contextHints = {};
        entries.forEach(([k, v]) => {
          obj.contextHints[k] = v;
        });
      }
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.maxTokens !== 0) {
      obj.maxTokens = Math.round(message.maxTokens);
    }
    if (message.temperature !== 0) {
      obj.temperature = message.temperature;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<PromptGenerationRequest>, I>>(base?: I): PromptGenerationRequest {
    return PromptGenerationRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<PromptGenerationRequest>, I>>(object: I): PromptGenerationRequest {
    const message = createBasePromptGenerationRequest();
    message.basePrompt = object.basePrompt ?? "";
    message.analysisType = object.analysisType ?? "";
    message.contextTypes = object.contextTypes?.map((e) => e) || [];
    message.maxContextItems = object.maxContextItems ?? 0;
    message.contextHints = Object.entries(object.contextHints ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    message.model = object.model ?? "";
    message.maxTokens = object.maxTokens ?? 0;
    message.temperature = object.temperature ?? 0;
    return message;
  },
};

function createBasePromptGenerationRequest_ContextHintsEntry(): PromptGenerationRequest_ContextHintsEntry {
  return { key: "", value: "" };
}

export const PromptGenerationRequest_ContextHintsEntry = {
  encode(message: PromptGenerationRequest_ContextHintsEntry, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): PromptGenerationRequest_ContextHintsEntry {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePromptGenerationRequest_ContextHintsEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PromptGenerationRequest_ContextHintsEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: PromptGenerationRequest_ContextHintsEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<PromptGenerationRequest_ContextHintsEntry>, I>>(
    base?: I,
  ): PromptGenerationRequest_ContextHintsEntry {
    return PromptGenerationRequest_ContextHintsEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<PromptGenerationRequest_ContextHintsEntry>, I>>(
    object: I,
  ): PromptGenerationRequest_ContextHintsEntry {
    const message = createBasePromptGenerationRequest_ContextHintsEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBasePromptGenerationResponse(): PromptGenerationResponse {
  return {
    enhancedPrompt: "",
    contextItemsUsed: 0,
    contextItems: [],
    processingTime: 0,
    modelUsed: "",
    success: false,
    error: "",
  };
}

export const PromptGenerationResponse = {
  encode(message: PromptGenerationResponse, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.enhancedPrompt !== "") {
      writer.uint32(10).string(message.enhancedPrompt);
    }
    if (message.contextItemsUsed !== 0) {
      writer.uint32(16).int32(message.contextItemsUsed);
    }
    for (const v of message.contextItems) {
      ContextItem.encode(v!, writer.uint32(26).fork()).ldelim();
    }
    if (message.processingTime !== 0) {
      writer.uint32(33).double(message.processingTime);
    }
    if (message.modelUsed !== "") {
      writer.uint32(42).string(message.modelUsed);
    }
    if (message.success !== false) {
      writer.uint32(48).bool(message.success);
    }
    if (message.error !== "") {
      writer.uint32(58).string(message.error);
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): PromptGenerationResponse {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePromptGenerationResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.enhancedPrompt = reader.string();
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.contextItemsUsed = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.contextItems.push(ContextItem.decode(reader, reader.uint32()));
          continue;
        case 4:
          if (tag !== 33) {
            break;
          }

          message.processingTime = reader.double();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.modelUsed = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.success = reader.bool();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.error = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PromptGenerationResponse {
    return {
      enhancedPrompt: isSet(object.enhancedPrompt) ? globalThis.String(object.enhancedPrompt) : "",
      contextItemsUsed: isSet(object.contextItemsUsed) ? globalThis.Number(object.contextItemsUsed) : 0,
      contextItems: globalThis.Array.isArray(object?.contextItems)
        ? object.contextItems.map((e: any) => ContextItem.fromJSON(e))
        : [],
      processingTime: isSet(object.processingTime) ? globalThis.Number(object.processingTime) : 0,
      modelUsed: isSet(object.modelUsed) ? globalThis.String(object.modelUsed) : "",
      success: isSet(object.success) ? globalThis.Boolean(object.success) : false,
      error: isSet(object.error) ? globalThis.String(object.error) : "",
    };
  },

  toJSON(message: PromptGenerationResponse): unknown {
    const obj: any = {};
    if (message.enhancedPrompt !== "") {
      obj.enhancedPrompt = message.enhancedPrompt;
    }
    if (message.contextItemsUsed !== 0) {
      obj.contextItemsUsed = Math.round(message.contextItemsUsed);
    }
    if (message.contextItems?.length) {
      obj.contextItems = message.contextItems.map((e) => ContextItem.toJSON(e));
    }
    if (message.processingTime !== 0) {
      obj.processingTime = message.processingTime;
    }
    if (message.modelUsed !== "") {
      obj.modelUsed = message.modelUsed;
    }
    if (message.success !== false) {
      obj.success = message.success;
    }
    if (message.error !== "") {
      obj.error = message.error;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<PromptGenerationResponse>, I>>(base?: I): PromptGenerationResponse {
    return PromptGenerationResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<PromptGenerationResponse>, I>>(object: I): PromptGenerationResponse {
    const message = createBasePromptGenerationResponse();
    message.enhancedPrompt = object.enhancedPrompt ?? "";
    message.contextItemsUsed = object.contextItemsUsed ?? 0;
    message.contextItems = object.contextItems?.map((e) => ContextItem.fromPartial(e)) || [];
    message.processingTime = object.processingTime ?? 0;
    message.modelUsed = object.modelUsed ?? "";
    message.success = object.success ?? false;
    message.error = object.error ?? "";
    return message;
  },
};

function createBaseContextSearchRequest(): ContextSearchRequest {
  return { query: "", contextTypes: [], maxResults: 0, minRelevance: 0, filters: {} };
}

export const ContextSearchRequest = {
  encode(message: ContextSearchRequest, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.query !== "") {
      writer.uint32(10).string(message.query);
    }
    for (const v of message.contextTypes) {
      writer.uint32(18).string(v!);
    }
    if (message.maxResults !== 0) {
      writer.uint32(24).int32(message.maxResults);
    }
    if (message.minRelevance !== 0) {
      writer.uint32(37).float(message.minRelevance);
    }
    Object.entries(message.filters).forEach(([key, value]) => {
      ContextSearchRequest_FiltersEntry.encode({ key: key as any, value }, writer.uint32(42).fork()).ldelim();
    });
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): ContextSearchRequest {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContextSearchRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.query = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.contextTypes.push(reader.string());
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.maxResults = reader.int32();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.minRelevance = reader.float();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          const entry5 = ContextSearchRequest_FiltersEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            message.filters[entry5.key] = entry5.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContextSearchRequest {
    return {
      query: isSet(object.query) ? globalThis.String(object.query) : "",
      contextTypes: globalThis.Array.isArray(object?.contextTypes)
        ? object.contextTypes.map((e: any) => globalThis.String(e))
        : [],
      maxResults: isSet(object.maxResults) ? globalThis.Number(object.maxResults) : 0,
      minRelevance: isSet(object.minRelevance) ? globalThis.Number(object.minRelevance) : 0,
      filters: isObject(object.filters)
        ? Object.entries(object.filters).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: ContextSearchRequest): unknown {
    const obj: any = {};
    if (message.query !== "") {
      obj.query = message.query;
    }
    if (message.contextTypes?.length) {
      obj.contextTypes = message.contextTypes;
    }
    if (message.maxResults !== 0) {
      obj.maxResults = Math.round(message.maxResults);
    }
    if (message.minRelevance !== 0) {
      obj.minRelevance = message.minRelevance;
    }
    if (message.filters) {
      const entries = Object.entries(message.filters);
      if (entries.length > 0) {
        obj.filters = {};
        entries.forEach(([k, v]) => {
          obj.filters[k] = v;
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ContextSearchRequest>, I>>(base?: I): ContextSearchRequest {
    return ContextSearchRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ContextSearchRequest>, I>>(object: I): ContextSearchRequest {
    const message = createBaseContextSearchRequest();
    message.query = object.query ?? "";
    message.contextTypes = object.contextTypes?.map((e) => e) || [];
    message.maxResults = object.maxResults ?? 0;
    message.minRelevance = object.minRelevance ?? 0;
    message.filters = Object.entries(object.filters ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseContextSearchRequest_FiltersEntry(): ContextSearchRequest_FiltersEntry {
  return { key: "", value: "" };
}

export const ContextSearchRequest_FiltersEntry = {
  encode(message: ContextSearchRequest_FiltersEntry, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): ContextSearchRequest_FiltersEntry {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContextSearchRequest_FiltersEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContextSearchRequest_FiltersEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: ContextSearchRequest_FiltersEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ContextSearchRequest_FiltersEntry>, I>>(
    base?: I,
  ): ContextSearchRequest_FiltersEntry {
    return ContextSearchRequest_FiltersEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ContextSearchRequest_FiltersEntry>, I>>(
    object: I,
  ): ContextSearchRequest_FiltersEntry {
    const message = createBaseContextSearchRequest_FiltersEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseContextSearchResponse(): ContextSearchResponse {
  return { results: [], totalResults: 0, query: "", processingTime: 0 };
}

export const ContextSearchResponse = {
  encode(message: ContextSearchResponse, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    for (const v of message.results) {
      ContextItem.encode(v!, writer.uint32(10).fork()).ldelim();
    }
    if (message.totalResults !== 0) {
      writer.uint32(16).int32(message.totalResults);
    }
    if (message.query !== "") {
      writer.uint32(26).string(message.query);
    }
    if (message.processingTime !== 0) {
      writer.uint32(33).double(message.processingTime);
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): ContextSearchResponse {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContextSearchResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.results.push(ContextItem.decode(reader, reader.uint32()));
          continue;
        case 2:
          if (tag !== 16) {
            break;
          }

          message.totalResults = reader.int32();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.query = reader.string();
          continue;
        case 4:
          if (tag !== 33) {
            break;
          }

          message.processingTime = reader.double();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContextSearchResponse {
    return {
      results: globalThis.Array.isArray(object?.results) ? object.results.map((e: any) => ContextItem.fromJSON(e)) : [],
      totalResults: isSet(object.totalResults) ? globalThis.Number(object.totalResults) : 0,
      query: isSet(object.query) ? globalThis.String(object.query) : "",
      processingTime: isSet(object.processingTime) ? globalThis.Number(object.processingTime) : 0,
    };
  },

  toJSON(message: ContextSearchResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => ContextItem.toJSON(e));
    }
    if (message.totalResults !== 0) {
      obj.totalResults = Math.round(message.totalResults);
    }
    if (message.query !== "") {
      obj.query = message.query;
    }
    if (message.processingTime !== 0) {
      obj.processingTime = message.processingTime;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ContextSearchResponse>, I>>(base?: I): ContextSearchResponse {
    return ContextSearchResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ContextSearchResponse>, I>>(object: I): ContextSearchResponse {
    const message = createBaseContextSearchResponse();
    message.results = object.results?.map((e) => ContextItem.fromPartial(e)) || [];
    message.totalResults = object.totalResults ?? 0;
    message.query = object.query ?? "";
    message.processingTime = object.processingTime ?? 0;
    return message;
  },
};

function createBaseTextGenerationRequest(): TextGenerationRequest {
  return { prompt: "", model: "", maxTokens: 0, temperature: 0, topP: 0, stopSequences: [], parameters: {} };
}

export const TextGenerationRequest = {
  encode(message: TextGenerationRequest, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.prompt !== "") {
      writer.uint32(10).string(message.prompt);
    }
    if (message.model !== "") {
      writer.uint32(18).string(message.model);
    }
    if (message.maxTokens !== 0) {
      writer.uint32(24).int32(message.maxTokens);
    }
    if (message.temperature !== 0) {
      writer.uint32(37).float(message.temperature);
    }
    if (message.topP !== 0) {
      writer.uint32(45).float(message.topP);
    }
    for (const v of message.stopSequences) {
      writer.uint32(50).string(v!);
    }
    Object.entries(message.parameters).forEach(([key, value]) => {
      TextGenerationRequest_ParametersEntry.encode({ key: key as any, value }, writer.uint32(58).fork()).ldelim();
    });
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): TextGenerationRequest {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextGenerationRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.prompt = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.model = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.maxTokens = reader.int32();
          continue;
        case 4:
          if (tag !== 37) {
            break;
          }

          message.temperature = reader.float();
          continue;
        case 5:
          if (tag !== 45) {
            break;
          }

          message.topP = reader.float();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.stopSequences.push(reader.string());
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          const entry7 = TextGenerationRequest_ParametersEntry.decode(reader, reader.uint32());
          if (entry7.value !== undefined) {
            message.parameters[entry7.key] = entry7.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextGenerationRequest {
    return {
      prompt: isSet(object.prompt) ? globalThis.String(object.prompt) : "",
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      maxTokens: isSet(object.maxTokens) ? globalThis.Number(object.maxTokens) : 0,
      temperature: isSet(object.temperature) ? globalThis.Number(object.temperature) : 0,
      topP: isSet(object.topP) ? globalThis.Number(object.topP) : 0,
      stopSequences: globalThis.Array.isArray(object?.stopSequences)
        ? object.stopSequences.map((e: any) => globalThis.String(e))
        : [],
      parameters: isObject(object.parameters)
        ? Object.entries(object.parameters).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: TextGenerationRequest): unknown {
    const obj: any = {};
    if (message.prompt !== "") {
      obj.prompt = message.prompt;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.maxTokens !== 0) {
      obj.maxTokens = Math.round(message.maxTokens);
    }
    if (message.temperature !== 0) {
      obj.temperature = message.temperature;
    }
    if (message.topP !== 0) {
      obj.topP = message.topP;
    }
    if (message.stopSequences?.length) {
      obj.stopSequences = message.stopSequences;
    }
    if (message.parameters) {
      const entries = Object.entries(message.parameters);
      if (entries.length > 0) {
        obj.parameters = {};
        entries.forEach(([k, v]) => {
          obj.parameters[k] = v;
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TextGenerationRequest>, I>>(base?: I): TextGenerationRequest {
    return TextGenerationRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TextGenerationRequest>, I>>(object: I): TextGenerationRequest {
    const message = createBaseTextGenerationRequest();
    message.prompt = object.prompt ?? "";
    message.model = object.model ?? "";
    message.maxTokens = object.maxTokens ?? 0;
    message.temperature = object.temperature ?? 0;
    message.topP = object.topP ?? 0;
    message.stopSequences = object.stopSequences?.map((e) => e) || [];
    message.parameters = Object.entries(object.parameters ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseTextGenerationRequest_ParametersEntry(): TextGenerationRequest_ParametersEntry {
  return { key: "", value: "" };
}

export const TextGenerationRequest_ParametersEntry = {
  encode(message: TextGenerationRequest_ParametersEntry, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): TextGenerationRequest_ParametersEntry {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextGenerationRequest_ParametersEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextGenerationRequest_ParametersEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: TextGenerationRequest_ParametersEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TextGenerationRequest_ParametersEntry>, I>>(
    base?: I,
  ): TextGenerationRequest_ParametersEntry {
    return TextGenerationRequest_ParametersEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TextGenerationRequest_ParametersEntry>, I>>(
    object: I,
  ): TextGenerationRequest_ParametersEntry {
    const message = createBaseTextGenerationRequest_ParametersEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseTextGenerationResponse(): TextGenerationResponse {
  return { text: "", modelUsed: "", tokensGenerated: 0, processingTime: 0, success: false, error: "" };
}

export const TextGenerationResponse = {
  encode(message: TextGenerationResponse, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.text !== "") {
      writer.uint32(10).string(message.text);
    }
    if (message.modelUsed !== "") {
      writer.uint32(18).string(message.modelUsed);
    }
    if (message.tokensGenerated !== 0) {
      writer.uint32(24).int32(message.tokensGenerated);
    }
    if (message.processingTime !== 0) {
      writer.uint32(33).double(message.processingTime);
    }
    if (message.success !== false) {
      writer.uint32(40).bool(message.success);
    }
    if (message.error !== "") {
      writer.uint32(50).string(message.error);
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): TextGenerationResponse {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextGenerationResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.text = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.modelUsed = reader.string();
          continue;
        case 3:
          if (tag !== 24) {
            break;
          }

          message.tokensGenerated = reader.int32();
          continue;
        case 4:
          if (tag !== 33) {
            break;
          }

          message.processingTime = reader.double();
          continue;
        case 5:
          if (tag !== 40) {
            break;
          }

          message.success = reader.bool();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.error = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextGenerationResponse {
    return {
      text: isSet(object.text) ? globalThis.String(object.text) : "",
      modelUsed: isSet(object.modelUsed) ? globalThis.String(object.modelUsed) : "",
      tokensGenerated: isSet(object.tokensGenerated) ? globalThis.Number(object.tokensGenerated) : 0,
      processingTime: isSet(object.processingTime) ? globalThis.Number(object.processingTime) : 0,
      success: isSet(object.success) ? globalThis.Boolean(object.success) : false,
      error: isSet(object.error) ? globalThis.String(object.error) : "",
    };
  },

  toJSON(message: TextGenerationResponse): unknown {
    const obj: any = {};
    if (message.text !== "") {
      obj.text = message.text;
    }
    if (message.modelUsed !== "") {
      obj.modelUsed = message.modelUsed;
    }
    if (message.tokensGenerated !== 0) {
      obj.tokensGenerated = Math.round(message.tokensGenerated);
    }
    if (message.processingTime !== 0) {
      obj.processingTime = message.processingTime;
    }
    if (message.success !== false) {
      obj.success = message.success;
    }
    if (message.error !== "") {
      obj.error = message.error;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TextGenerationResponse>, I>>(base?: I): TextGenerationResponse {
    return TextGenerationResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TextGenerationResponse>, I>>(object: I): TextGenerationResponse {
    const message = createBaseTextGenerationResponse();
    message.text = object.text ?? "";
    message.modelUsed = object.modelUsed ?? "";
    message.tokensGenerated = object.tokensGenerated ?? 0;
    message.processingTime = object.processingTime ?? 0;
    message.success = object.success ?? false;
    message.error = object.error ?? "";
    return message;
  },
};

function createBaseContextItem(): ContextItem {
  return {
    id: "",
    type: "",
    title: "",
    content: "",
    filePath: "",
    tags: [],
    relevanceScore: 0,
    lastModified: "0",
    metadata: {},
  };
}

export const ContextItem = {
  encode(message: ContextItem, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.id !== "") {
      writer.uint32(10).string(message.id);
    }
    if (message.type !== "") {
      writer.uint32(18).string(message.type);
    }
    if (message.title !== "") {
      writer.uint32(26).string(message.title);
    }
    if (message.content !== "") {
      writer.uint32(34).string(message.content);
    }
    if (message.filePath !== "") {
      writer.uint32(42).string(message.filePath);
    }
    for (const v of message.tags) {
      writer.uint32(50).string(v!);
    }
    if (message.relevanceScore !== 0) {
      writer.uint32(57).double(message.relevanceScore);
    }
    if (message.lastModified !== "0") {
      writer.uint32(64).int64(message.lastModified);
    }
    Object.entries(message.metadata).forEach(([key, value]) => {
      ContextItem_MetadataEntry.encode({ key: key as any, value }, writer.uint32(74).fork()).ldelim();
    });
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): ContextItem {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContextItem();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.id = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.type = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.title = reader.string();
          continue;
        case 4:
          if (tag !== 34) {
            break;
          }

          message.content = reader.string();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.filePath = reader.string();
          continue;
        case 6:
          if (tag !== 50) {
            break;
          }

          message.tags.push(reader.string());
          continue;
        case 7:
          if (tag !== 57) {
            break;
          }

          message.relevanceScore = reader.double();
          continue;
        case 8:
          if (tag !== 64) {
            break;
          }

          message.lastModified = longToString(reader.int64() as Long);
          continue;
        case 9:
          if (tag !== 74) {
            break;
          }

          const entry9 = ContextItem_MetadataEntry.decode(reader, reader.uint32());
          if (entry9.value !== undefined) {
            message.metadata[entry9.key] = entry9.value;
          }
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContextItem {
    return {
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      type: isSet(object.type) ? globalThis.String(object.type) : "",
      title: isSet(object.title) ? globalThis.String(object.title) : "",
      content: isSet(object.content) ? globalThis.String(object.content) : "",
      filePath: isSet(object.filePath) ? globalThis.String(object.filePath) : "",
      tags: globalThis.Array.isArray(object?.tags) ? object.tags.map((e: any) => globalThis.String(e)) : [],
      relevanceScore: isSet(object.relevanceScore) ? globalThis.Number(object.relevanceScore) : 0,
      lastModified: isSet(object.lastModified) ? globalThis.String(object.lastModified) : "0",
      metadata: isObject(object.metadata)
        ? Object.entries(object.metadata).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: ContextItem): unknown {
    const obj: any = {};
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.type !== "") {
      obj.type = message.type;
    }
    if (message.title !== "") {
      obj.title = message.title;
    }
    if (message.content !== "") {
      obj.content = message.content;
    }
    if (message.filePath !== "") {
      obj.filePath = message.filePath;
    }
    if (message.tags?.length) {
      obj.tags = message.tags;
    }
    if (message.relevanceScore !== 0) {
      obj.relevanceScore = message.relevanceScore;
    }
    if (message.lastModified !== "0") {
      obj.lastModified = message.lastModified;
    }
    if (message.metadata) {
      const entries = Object.entries(message.metadata);
      if (entries.length > 0) {
        obj.metadata = {};
        entries.forEach(([k, v]) => {
          obj.metadata[k] = v;
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ContextItem>, I>>(base?: I): ContextItem {
    return ContextItem.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ContextItem>, I>>(object: I): ContextItem {
    const message = createBaseContextItem();
    message.id = object.id ?? "";
    message.type = object.type ?? "";
    message.title = object.title ?? "";
    message.content = object.content ?? "";
    message.filePath = object.filePath ?? "";
    message.tags = object.tags?.map((e) => e) || [];
    message.relevanceScore = object.relevanceScore ?? 0;
    message.lastModified = object.lastModified ?? "0";
    message.metadata = Object.entries(object.metadata ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseContextItem_MetadataEntry(): ContextItem_MetadataEntry {
  return { key: "", value: "" };
}

export const ContextItem_MetadataEntry = {
  encode(message: ContextItem_MetadataEntry, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): ContextItem_MetadataEntry {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseContextItem_MetadataEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ContextItem_MetadataEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: ContextItem_MetadataEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ContextItem_MetadataEntry>, I>>(base?: I): ContextItem_MetadataEntry {
    return ContextItem_MetadataEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ContextItem_MetadataEntry>, I>>(object: I): ContextItem_MetadataEntry {
    const message = createBaseContextItem_MetadataEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseLLMModelsResponse(): LLMModelsResponse {
  return { models: [] };
}

export const LLMModelsResponse = {
  encode(message: LLMModelsResponse, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    for (const v of message.models) {
      LLMModelInfo.encode(v!, writer.uint32(10).fork()).ldelim();
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): LLMModelsResponse {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLLMModelsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.models.push(LLMModelInfo.decode(reader, reader.uint32()));
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LLMModelsResponse {
    return {
      models: globalThis.Array.isArray(object?.models) ? object.models.map((e: any) => LLMModelInfo.fromJSON(e)) : [],
    };
  },

  toJSON(message: LLMModelsResponse): unknown {
    const obj: any = {};
    if (message.models?.length) {
      obj.models = message.models.map((e) => LLMModelInfo.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<LLMModelsResponse>, I>>(base?: I): LLMModelsResponse {
    return LLMModelsResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<LLMModelsResponse>, I>>(object: I): LLMModelsResponse {
    const message = createBaseLLMModelsResponse();
    message.models = object.models?.map((e) => LLMModelInfo.fromPartial(e)) || [];
    return message;
  },
};

function createBaseLLMModelInfo(): LLMModelInfo {
  return { name: "", displayName: "", description: "", available: false, provider: "", maxTokens: 0, capabilities: [] };
}

export const LLMModelInfo = {
  encode(message: LLMModelInfo, writer: _m0.Writer = _m0.Writer.create()): _m0.Writer {
    if (message.name !== "") {
      writer.uint32(10).string(message.name);
    }
    if (message.displayName !== "") {
      writer.uint32(18).string(message.displayName);
    }
    if (message.description !== "") {
      writer.uint32(26).string(message.description);
    }
    if (message.available !== false) {
      writer.uint32(32).bool(message.available);
    }
    if (message.provider !== "") {
      writer.uint32(42).string(message.provider);
    }
    if (message.maxTokens !== 0) {
      writer.uint32(48).int32(message.maxTokens);
    }
    for (const v of message.capabilities) {
      writer.uint32(58).string(v!);
    }
    return writer;
  },

  decode(input: _m0.Reader | Uint8Array, length?: number): LLMModelInfo {
    const reader = input instanceof _m0.Reader ? input : _m0.Reader.create(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLLMModelInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1:
          if (tag !== 10) {
            break;
          }

          message.name = reader.string();
          continue;
        case 2:
          if (tag !== 18) {
            break;
          }

          message.displayName = reader.string();
          continue;
        case 3:
          if (tag !== 26) {
            break;
          }

          message.description = reader.string();
          continue;
        case 4:
          if (tag !== 32) {
            break;
          }

          message.available = reader.bool();
          continue;
        case 5:
          if (tag !== 42) {
            break;
          }

          message.provider = reader.string();
          continue;
        case 6:
          if (tag !== 48) {
            break;
          }

          message.maxTokens = reader.int32();
          continue;
        case 7:
          if (tag !== 58) {
            break;
          }

          message.capabilities.push(reader.string());
          continue;
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skipType(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): LLMModelInfo {
    return {
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      displayName: isSet(object.displayName) ? globalThis.String(object.displayName) : "",
      description: isSet(object.description) ? globalThis.String(object.description) : "",
      available: isSet(object.available) ? globalThis.Boolean(object.available) : false,
      provider: isSet(object.provider) ? globalThis.String(object.provider) : "",
      maxTokens: isSet(object.maxTokens) ? globalThis.Number(object.maxTokens) : 0,
      capabilities: globalThis.Array.isArray(object?.capabilities)
        ? object.capabilities.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: LLMModelInfo): unknown {
    const obj: any = {};
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.displayName !== "") {
      obj.displayName = message.displayName;
    }
    if (message.description !== "") {
      obj.description = message.description;
    }
    if (message.available !== false) {
      obj.available = message.available;
    }
    if (message.provider !== "") {
      obj.provider = message.provider;
    }
    if (message.maxTokens !== 0) {
      obj.maxTokens = Math.round(message.maxTokens);
    }
    if (message.capabilities?.length) {
      obj.capabilities = message.capabilities;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<LLMModelInfo>, I>>(base?: I): LLMModelInfo {
    return LLMModelInfo.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<LLMModelInfo>, I>>(object: I): LLMModelInfo {
    const message = createBaseLLMModelInfo();
    message.name = object.name ?? "";
    message.displayName = object.displayName ?? "";
    message.description = object.description ?? "";
    message.available = object.available ?? false;
    message.provider = object.provider ?? "";
    message.maxTokens = object.maxTokens ?? 0;
    message.capabilities = object.capabilities?.map((e) => e) || [];
    return message;
  },
};

/** Context-Aware LLM Service - Pure LLM operations for prompt enhancement */
export type ContextServiceService = typeof ContextServiceService;
export const ContextServiceService = {
  /** Generate enhanced prompt with project context */
  generatePrompt: {
    path: "/multimodal.ContextService/GeneratePrompt",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: PromptGenerationRequest) => Buffer.from(PromptGenerationRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => PromptGenerationRequest.decode(value),
    responseSerialize: (value: PromptGenerationResponse) =>
      Buffer.from(PromptGenerationResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => PromptGenerationResponse.decode(value),
  },
  /** Search project context and documentation */
  searchContext: {
    path: "/multimodal.ContextService/SearchContext",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: ContextSearchRequest) => Buffer.from(ContextSearchRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => ContextSearchRequest.decode(value),
    responseSerialize: (value: ContextSearchResponse) => Buffer.from(ContextSearchResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => ContextSearchResponse.decode(value),
  },
  /** Generate text using LLM */
  generateText: {
    path: "/multimodal.ContextService/GenerateText",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: TextGenerationRequest) => Buffer.from(TextGenerationRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => TextGenerationRequest.decode(value),
    responseSerialize: (value: TextGenerationResponse) => Buffer.from(TextGenerationResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => TextGenerationResponse.decode(value),
  },
  /** Get available LLM models */
  getAvailableModels: {
    path: "/multimodal.ContextService/GetAvailableModels",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: Empty) => Buffer.from(Empty.encode(value).finish()),
    requestDeserialize: (value: Buffer) => Empty.decode(value),
    responseSerialize: (value: LLMModelsResponse) => Buffer.from(LLMModelsResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => LLMModelsResponse.decode(value),
  },
  /** Health check for service availability */
  getHealth: {
    path: "/multimodal.ContextService/GetHealth",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: Empty) => Buffer.from(Empty.encode(value).finish()),
    requestDeserialize: (value: Buffer) => Empty.decode(value),
    responseSerialize: (value: HealthResponse) => Buffer.from(HealthResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => HealthResponse.decode(value),
  },
} as const;

export interface ContextServiceServer extends UntypedServiceImplementation {
  /** Generate enhanced prompt with project context */
  generatePrompt: handleUnaryCall<PromptGenerationRequest, PromptGenerationResponse>;
  /** Search project context and documentation */
  searchContext: handleUnaryCall<ContextSearchRequest, ContextSearchResponse>;
  /** Generate text using LLM */
  generateText: handleUnaryCall<TextGenerationRequest, TextGenerationResponse>;
  /** Get available LLM models */
  getAvailableModels: handleUnaryCall<Empty, LLMModelsResponse>;
  /** Health check for service availability */
  getHealth: handleUnaryCall<Empty, HealthResponse>;
}

export interface ContextServiceClient extends Client {
  /** Generate enhanced prompt with project context */
  generatePrompt(
    request: PromptGenerationRequest,
    callback: (error: ServiceError | null, response: PromptGenerationResponse) => void,
  ): ClientUnaryCall;
  generatePrompt(
    request: PromptGenerationRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: PromptGenerationResponse) => void,
  ): ClientUnaryCall;
  generatePrompt(
    request: PromptGenerationRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: PromptGenerationResponse) => void,
  ): ClientUnaryCall;
  /** Search project context and documentation */
  searchContext(
    request: ContextSearchRequest,
    callback: (error: ServiceError | null, response: ContextSearchResponse) => void,
  ): ClientUnaryCall;
  searchContext(
    request: ContextSearchRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: ContextSearchResponse) => void,
  ): ClientUnaryCall;
  searchContext(
    request: ContextSearchRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: ContextSearchResponse) => void,
  ): ClientUnaryCall;
  /** Generate text using LLM */
  generateText(
    request: TextGenerationRequest,
    callback: (error: ServiceError | null, response: TextGenerationResponse) => void,
  ): ClientUnaryCall;
  generateText(
    request: TextGenerationRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: TextGenerationResponse) => void,
  ): ClientUnaryCall;
  generateText(
    request: TextGenerationRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: TextGenerationResponse) => void,
  ): ClientUnaryCall;
  /** Get available LLM models */
  getAvailableModels(
    request: Empty,
    callback: (error: ServiceError | null, response: LLMModelsResponse) => void,
  ): ClientUnaryCall;
  getAvailableModels(
    request: Empty,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: LLMModelsResponse) => void,
  ): ClientUnaryCall;
  getAvailableModels(
    request: Empty,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: LLMModelsResponse) => void,
  ): ClientUnaryCall;
  /** Health check for service availability */
  getHealth(request: Empty, callback: (error: ServiceError | null, response: HealthResponse) => void): ClientUnaryCall;
  getHealth(
    request: Empty,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: HealthResponse) => void,
  ): ClientUnaryCall;
  getHealth(
    request: Empty,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: HealthResponse) => void,
  ): ClientUnaryCall;
}

export const ContextServiceClient = makeGenericClientConstructor(
  ContextServiceService,
  "multimodal.ContextService",
) as unknown as {
  new (address: string, credentials: ChannelCredentials, options?: Partial<ClientOptions>): ContextServiceClient;
  service: typeof ContextServiceService;
  serviceName: string;
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function longToString(long: Long) {
  return long.toString();
}

if (_m0.util.Long !== Long) {
  _m0.util.Long = Long as any;
  _m0.configure();
}

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}
