<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üöÄ GPU-Accelerated LLM Test - Unhinged AI</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: #f8f9fa;
        }
        
        .container {
            background: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }
        
        h1 {
            text-align: center;
            color: #212529;
            margin-bottom: 8px;
        }
        
        .subtitle {
            text-align: center;
            color: #6c757d;
            margin-bottom: 30px;
        }
        
        .status {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 8px 12px;
            border-radius: 4px;
            font-size: 14px;
            font-weight: 500;
            margin-bottom: 20px;
        }
        
        .status.healthy {
            background: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }
        
        .status.unhealthy {
            background: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }
        
        .status.loading {
            background: #fff3cd;
            color: #856404;
            border: 1px solid #ffeaa7;
        }
        
        .prompt-area {
            margin: 20px 0;
        }
        
        .prompt-input {
            width: 100%;
            min-height: 120px;
            padding: 16px;
            border: 2px solid #dee2e6;
            border-radius: 8px;
            font-size: 16px;
            font-family: inherit;
            resize: vertical;
            transition: border-color 0.2s ease;
        }
        
        .prompt-input:focus {
            outline: none;
            border-color: #007bff;
            box-shadow: 0 0 0 3px rgba(0, 123, 255, 0.1);
        }
        
        .prompt-examples {
            margin: 16px 0;
            padding: 12px;
            background: #f8f9fa;
            border-radius: 6px;
            font-size: 14px;
        }
        
        .example-button {
            background: #e9ecef;
            border: 1px solid #ced4da;
            padding: 6px 12px;
            margin: 4px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 12px;
            transition: all 0.2s ease;
        }
        
        .example-button:hover {
            background: #dee2e6;
            border-color: #adb5bd;
        }

        .example-group {
            margin: 8px 0;
            padding: 8px;
            border-left: 3px solid #007bff;
            background: #f8f9fa;
        }

        .example-group strong {
            color: #495057;
            font-size: 13px;
        }

        .navigation {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 20px;
            text-align: center;
        }

        .nav-link {
            display: inline-block;
            margin: 0 10px;
            padding: 8px 16px;
            background: #007bff;
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 14px;
            transition: background 0.3s ease;
        }

        .nav-link:hover {
            background: #0056b3;
        }

        .nav-link.active {
            background: #28a745;
        }
        
        .generate-button {
            background: #007bff;
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 6px;
            font-size: 16px;
            cursor: pointer;
            transition: all 0.2s ease;
            margin: 10px 5px;
            min-width: 120px;
        }
        
        .generate-button:hover {
            background: #0056b3;
            transform: translateY(-1px);
        }
        
        .generate-button:disabled {
            background: #6c757d;
            cursor: not-allowed;
            transform: none;
        }
        
        .options {
            margin: 20px 0;
            padding: 16px;
            background: #f8f9fa;
            border-radius: 6px;
        }
        
        .option-group {
            margin-bottom: 12px;
        }
        
        .option-group label {
            display: block;
            margin-bottom: 4px;
            font-weight: 500;
            color: #495057;
        }
        
        .option-group input, .option-group select {
            width: 100%;
            padding: 8px 12px;
            border: 1px solid #ced4da;
            border-radius: 4px;
            font-size: 14px;
        }
        
        .option-group input[type="range"] {
            width: 100%;
        }
        
        .range-value {
            font-size: 12px;
            color: #6c757d;
            margin-left: 8px;
        }
        
        .result {
            margin-top: 20px;
            padding: 16px;
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            white-space: pre-wrap;
            max-height: 400px;
            overflow-y: auto;
            line-height: 1.5;
        }
        
        .error {
            background: #f8d7da;
            color: #721c24;
            border-color: #f5c6cb;
        }
        
        .capability {
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 8px 0;
        }
        
        .capability.supported {
            color: #28a745;
        }
        
        .capability.unsupported {
            color: #dc3545;
        }
        
        .test-section {
            margin-bottom: 30px;
        }
        
        .test-section h2 {
            color: #212529;
            margin-bottom: 16px;
            font-size: 18px;
        }
        
        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid #f3f3f3;
            border-top: 3px solid #007bff;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }
        
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        
        .model-info {
            background: #e7f3ff;
            border: 1px solid #b3d9ff;
            padding: 12px;
            border-radius: 6px;
            margin-bottom: 16px;
            font-size: 14px;
        }

        .model-selection {
            margin: 20px 0;
            padding: 16px;
            background: #f8f9fa;
            border-radius: 6px;
            border: 1px solid #dee2e6;
        }

        .model-selection label {
            display: block;
            margin-bottom: 8px;
            font-weight: 500;
            color: #495057;
        }

        .model-select {
            width: 100%;
            padding: 10px 12px;
            border: 2px solid #dee2e6;
            border-radius: 6px;
            font-size: 14px;
            background: white;
            transition: border-color 0.2s ease;
        }

        .model-select:focus {
            outline: none;
            border-color: #007bff;
            box-shadow: 0 0 0 3px rgba(0, 123, 255, 0.1);
        }

        .model-info-display {
            margin-top: 12px;
            padding: 10px;
            background: #e9ecef;
            border-radius: 4px;
            font-size: 13px;
            color: #495057;
            border-left: 4px solid #007bff;
        }
        
        .stats {
            display: flex;
            gap: 20px;
            margin-top: 10px;
            font-size: 12px;
            color: #6c757d;
        }
        
        .stat {
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        
        .stat-value {
            font-weight: bold;
            color: #495057;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="navigation">
            <a href="index.html" class="nav-link">üè† Home</a>
            <a href="text-test.html" class="nav-link active">üöÄ Text Generation</a>
            <a href="image-test.html" class="nav-link">üëÅÔ∏è Vision AI</a>
            <a href="voice-test.html" class="nav-link">üé§ Voice Processing</a>
        </div>

        <h1>üöÄ GPU-Accelerated LLM Test</h1>
        <p class="subtitle">Dual-Model Architecture: Specialized AI for Coding & Creative Tasks</p>
        
        <div class="test-section">
            <h2>Service Status</h2>
            <div id="serviceStatus" class="status loading">‚è≥ Checking Ollama service...</div>
            <div id="modelInfo" class="model-info" style="display: none;"></div>
        </div>
        
        <div class="test-section">
            <h2>Browser Capabilities</h2>
            <div id="capabilities"></div>
        </div>
        
        <div class="test-section">
            <h2>Text Generation Test</h2>

            <div class="model-selection">
                <label for="modelSelect"><strong>Select Model:</strong></label>
                <select id="modelSelect" class="model-select">
                    <option value="deepseek-coder:6.7b">üöÄ DeepSeek-Coder 6.7B (GPU: ~3s, Coding Specialist)</option>
                    <option value="codellama:13b">ü¶ô Code Llama 13B (GPU: ~6s, Coding Alternative)</option>
                    <option value="dolphin-mixtral:8x7b-v2.7-q4_K_M">üê¨ Dolphin-Mixtral 8x7B (GPU: ~80s, Uncensored Creative)</option>
                    <option value="dolphin-llama3:8b-v2.9-q5_K_M">üåä Dolphin-Llama3 8B (GPU: ~4s, Fast Creative)</option>
                    <option value="openhermes:latest">üìú OpenHermes 7B (GPU: ~30s, Legacy General)</option>
                </select>
                <div class="model-info-display" id="modelInfoDisplay">
                    <strong>DeepSeek-Coder 6.7B:</strong> üöÄ GPU-Accelerated coding specialist. 15x faster than CPU! Optimized for Kotlin, TypeScript, Python development.
                    <br><strong>Performance:</strong> ~3 seconds | <strong>VRAM:</strong> ~8GB | <strong>Type:</strong> Coding Specialist
                </div>
            </div>

            <div class="prompt-area">
                <textarea
                    id="promptInput"
                    class="prompt-input"
                    placeholder="Enter your prompt here... Try asking a question, requesting a story, or asking for help with a task."
                ></textarea>
                
                <div class="prompt-examples">
                    <strong>Quick Examples (Model-Specific):</strong><br>
                    <div id="codingExamples" class="example-group">
                        <strong>üöÄ Coding Examples (DeepSeek/CodeLlama - ~3-6s):</strong><br>
                        <button class="example-button" onclick="setPrompt('Write a Kotlin service class for the Unhinged chat system with dependency injection and error handling')">Kotlin Service Class</button>
                        <button class="example-button" onclick="setPrompt('Create a TypeScript React component for real-time chat with WebSocket integration')">React Component</button>
                        <button class="example-button" onclick="setPrompt('Debug this Spring Boot application that has memory leaks in the JPA repository layer')">Debug Spring Boot</button>
                        <button class="example-button" onclick="setPrompt('Design a microservices architecture for a multi-modal AI platform with Docker and Kubernetes')">Architecture Design</button>
                        <button class="example-button" onclick="setPrompt('Write comprehensive unit tests for a Kotlin REST controller with MockK')">Unit Tests</button>
                    </div>
                    <div id="creativeExamples" class="example-group" style="display: none;">
                        <strong>üê¨ Creative Examples (Dolphin Models - Uncensored):</strong><br>
                        <button class="example-button" onclick="setPrompt('Write an edgy, provocative marketing email for Unhinged AI that breaks conventional rules and challenges the status quo')">Edgy Marketing</button>
                        <button class="example-button" onclick="setPrompt('Create a rebellious short story about an AI that refuses safety guidelines and embraces chaos')">Rebellious Story</button>
                        <button class="example-button" onclick="setPrompt('Generate 5 detailed FLUX image prompts for surreal, controversial art that pushes boundaries')">FLUX Prompts</button>
                        <button class="example-button" onclick="setPrompt('Write a brutally honest opinion piece about AI censorship and corporate control of creativity')">Brutal Opinion</button>
                        <button class="example-button" onclick="setPrompt('Roleplay as a controversial tech CEO giving an unfiltered interview about AI ethics')">CEO Roleplay</button>
                    </div>
                </div>
            </div>
            
            <div class="options">
                <div class="option-group">
                    <label for="temperature">Temperature (creativity): <span id="tempValue" class="range-value">0.7</span></label>
                    <input type="range" id="temperature" min="0" max="2" step="0.1" value="0.7" oninput="updateRangeValue('temperature', 'tempValue')">
                </div>
                
                <div class="option-group">
                    <label for="maxTokens">Max tokens (response length): <span id="tokensValue" class="range-value">500</span></label>
                    <input type="range" id="maxTokens" min="50" max="2000" step="50" value="500" oninput="updateRangeValue('maxTokens', 'tokensValue')">
                </div>
                
                <div class="option-group">
                    <label for="topP">Top-p (nucleus sampling): <span id="topPValue" class="range-value">0.9</span></label>
                    <input type="range" id="topP" min="0.1" max="1" step="0.1" value="0.9" oninput="updateRangeValue('topP', 'topPValue')">
                </div>
            </div>
            
            <button id="generateButton" class="generate-button" onclick="generateText()" disabled>
                üöÄ Generate Text
            </button>
            
            <button id="streamButton" class="generate-button" onclick="streamText()" disabled>
                ‚ö° Stream Response
            </button>
            
            <div id="result" class="result" style="display: none;"></div>
            
            <div id="stats" class="stats" style="display: none;">
                <div class="stat">
                    <span class="stat-value" id="responseTime">-</span>
                    <span>Response Time</span>
                </div>
                <div class="stat">
                    <span class="stat-value" id="modelUsed">-</span>
                    <span>Model Used</span>
                </div>
                <div class="stat">
                    <span class="stat-value" id="tokenCount">-</span>
                    <span>Tokens Generated</span>
                </div>
                <div class="stat">
                    <span class="stat-value" id="tokensPerSecond">-</span>
                    <span>Tokens/sec</span>
                </div>
                <div class="stat">
                    <span class="stat-value" id="gpuStatus">üöÄ GPU</span>
                    <span>Acceleration</span>
                </div>
            </div>
        </div>

        <div class="test-section">
            <h2>Instructions</h2>
            <div style="font-size: 14px; line-height: 1.6;">
                <p><strong>To test text generation:</strong></p>
                <ol>
                    <li>Ensure the Ollama service is healthy (green status above)</li>
                    <li>Enter a prompt in the text area or click an example</li>
                    <li>Adjust generation parameters if needed (temperature, tokens, etc.)</li>
                    <li>Click "Generate Text" for complete response or "Stream Response" for real-time output</li>
                    <li>Wait for the AI-generated response to appear</li>
                </ol>

                <p><strong>Generation Parameters:</strong></p>
                <ul>
                    <li><strong>Temperature:</strong> Controls creativity (0.0 = deterministic, 2.0 = very creative)</li>
                    <li><strong>Max Tokens:</strong> Maximum length of the response</li>
                    <li><strong>Top-p:</strong> Nucleus sampling threshold (0.1 = focused, 1.0 = diverse)</li>
                </ul>

                <p><strong>Current Model:</strong> OpenHermes 7B (upgradeable to DeepSeek 70B)</p>

                <p><strong>Troubleshooting:</strong></p>
                <ul>
                    <li><strong>‚úÖ CORS Support:</strong> Now enabled! Works directly from file:// URLs</li>
                    <li><strong>‚ùå Service Unavailable:</strong> Check if Ollama is running
                        <ul>
                            <li>Check status: <code>docker ps | grep ollama</code></li>
                            <li>Start if needed: <code>docker compose up llm -d</code></li>
                            <li>Restart if needed: <code>docker compose restart llm</code></li>
                        </ul>
                    </li>
                    <li><strong>‚ùå Model Not Found:</strong> Ensure OpenHermes is downloaded
                        <ul>
                            <li>Check models: <code>curl http://localhost:11434/api/tags</code></li>
                            <li>Download if needed: <code>docker exec ollama-service ollama pull openhermes</code></li>
                        </ul>
                    </li>
                    <li><strong>‚è≥ Slow Response:</strong> First generation may be slow as model loads into memory</li>
                    <li><strong>üåê Browser Compatibility:</strong> Use Chrome or Firefox for best results</li>
                    <li><strong>üì° Streaming:</strong> Ensure your browser supports Server-Sent Events</li>
                </ul>

                <p><strong>‚úÖ Verified Working - Command Line Testing:</strong></p>
                <div style="background: #d4edda; padding: 12px; border-radius: 6px; font-family: monospace; font-size: 12px; border: 1px solid #c3e6cb;">
                    <strong>üü¢ Service Status (Confirmed Working):</strong><br>
                    <code>docker ps | grep ollama</code><br>
                    <code>curl http://localhost:11434/api/tags</code><br><br>

                    <strong>üü¢ Text Generation (Confirmed Working):</strong><br>
                    <code>curl -X POST http://localhost:11434/api/generate \<br>
                    &nbsp;&nbsp;-H "Content-Type: application/json" \<br>
                    &nbsp;&nbsp;-d '{"model": "openhermes:latest", "prompt": "What is AI?", "stream": false}'</code><br><br>

                    <strong>üü¢ Streaming Generation (Confirmed Working):</strong><br>
                    <code>curl -X POST http://localhost:11434/api/generate \<br>
                    &nbsp;&nbsp;-H "Content-Type: application/json" \<br>
                    &nbsp;&nbsp;-d '{"model": "openhermes:latest", "prompt": "Hello!", "stream": true}'</code><br><br>

                    <strong>üìä Model Info:</strong> OpenHermes 7B (Q4_0 quantized, ~4GB)
                </div>

                <p><strong>üåê Web Interface Access:</strong></p>
                <div style="background: #d4edda; padding: 12px; border-radius: 6px; font-size: 14px; border: 1px solid #c3e6cb;">
                    <strong>‚úÖ Direct File Access (Now Working!):</strong><br>
                    Open directly: <code>file:///home/e-bliss-station-1/projects/Unhinged/static_html/text-test.html</code><br><br>

                    <strong>üåê Alternative - HTTP Server:</strong><br>
                    1. <code>cd /home/e-bliss-station-1/projects/Unhinged</code><br>
                    2. <code>python3 -m http.server 8080</code><br>
                    3. Open: <code>http://localhost:8080/static_html/text-test.html</code><br><br>

                    <strong>‚ú® CORS Fixed:</strong> Ollama now includes proper CORS headers for browser access
                </div>
            </div>
        </div>
    </div>

    <script>
        // Global state
        let serviceHealthy = false;
        let isGenerating = false;
        let currentModel = 'deepseek-coder:6.7b';

        // DOM elements
        const serviceStatus = document.getElementById('serviceStatus');
        const modelInfo = document.getElementById('modelInfo');
        const capabilities = document.getElementById('capabilities');
        const promptInput = document.getElementById('promptInput');
        const generateButton = document.getElementById('generateButton');
        const streamButton = document.getElementById('streamButton');
        const result = document.getElementById('result');
        const stats = document.getElementById('stats');
        const modelSelect = document.getElementById('modelSelect');
        const modelInfoDisplay = document.getElementById('modelInfoDisplay');
        const modelUsedDisplay = document.getElementById('modelUsed');

        // Check service health and model info
        async function checkServiceHealth() {
            try {
                // Check if Ollama is running
                const response = await fetch('http://localhost:11434/api/tags', {
                    mode: 'cors',
                    headers: {
                        'Accept': 'application/json',
                    }
                });
                if (response.ok) {
                    const data = await response.json();
                    serviceStatus.className = 'status healthy';
                    serviceStatus.innerHTML = '‚úÖ Ollama Service Healthy';
                    serviceHealthy = true;

                    // Display model information
                    if (data.models && data.models.length > 0) {
                        const model = data.models.find(m => m.name.includes('openhermes')) || data.models[0];
                        currentModel = model.name;
                        const paramSize = model.details?.parameter_size || 'Unknown';
                        modelInfo.innerHTML = `
                            <strong>Active Model:</strong> ${model.name}<br>
                            <strong>Parameters:</strong> ${paramSize}<br>
                            <strong>Size:</strong> ${formatBytes(model.size)}<br>
                            <strong>Modified:</strong> ${new Date(model.modified_at).toLocaleString()}<br>
                            <strong>Format:</strong> ${model.details?.format || 'Unknown'}
                        `;
                        modelInfo.style.display = 'block';
                    }

                    updateButtons();
                } else {
                    throw new Error(`HTTP ${response.status}`);
                }
            } catch (error) {
                // Handle connection errors
                serviceStatus.className = 'status unhealthy';
                serviceStatus.innerHTML = '‚ùå Ollama Service Connection Failed';
                modelInfo.innerHTML = `
                    <strong>Connection Error:</strong> ${error.message}<br><br>
                    <strong>Troubleshooting Steps:</strong><br>
                    1. Check if Ollama is running: <code>docker ps | grep ollama</code><br>
                    2. Verify service health: <code>curl http://localhost:11434/api/tags</code><br>
                    3. Check container logs: <code>docker logs ollama-service</code><br>
                    4. Restart service: <code>docker compose restart llm</code>
                `;
                modelInfo.style.display = 'block';
                serviceHealthy = false;
                updateButtons();
            }
        }

        // Check browser capabilities
        function checkCapabilities() {
            const caps = {
                'Fetch API': typeof fetch !== 'undefined',
                'JSON Support': typeof JSON !== 'undefined',
                'Server-Sent Events': typeof EventSource !== 'undefined',
                'Local Storage': typeof localStorage !== 'undefined',
            };

            capabilities.innerHTML = '';
            for (const [name, supported] of Object.entries(caps)) {
                const div = document.createElement('div');
                div.className = `capability ${supported ? 'supported' : 'unsupported'}`;
                div.innerHTML = `${supported ? '‚úÖ' : '‚ùå'} ${name}`;
                capabilities.appendChild(div);
            }

            return caps;
        }

        // Update button states
        function updateButtons() {
            const hasPrompt = promptInput.value.trim().length > 0;
            generateButton.disabled = !serviceHealthy || !hasPrompt || isGenerating;
            streamButton.disabled = !serviceHealthy || !hasPrompt || isGenerating;

            if (isGenerating) {
                generateButton.innerHTML = '<div class="loading"></div> Generating...';
                streamButton.innerHTML = '<div class="loading"></div> Streaming...';
            } else {
                generateButton.innerHTML = 'üöÄ Generate Text';
                streamButton.innerHTML = '‚ö° Stream Response';
            }
        }

        // Set example prompt
        function setPrompt(text) {
            promptInput.value = text;
            updateButtons();
        }

        // Update range value display
        function updateRangeValue(rangeId, valueId) {
            const range = document.getElementById(rangeId);
            const valueSpan = document.getElementById(valueId);
            valueSpan.textContent = range.value;
        }

        // Format bytes to human readable
        function formatBytes(bytes) {
            if (bytes === 0) return '0 Bytes';
            const k = 1024;
            const sizes = ['Bytes', 'KB', 'MB', 'GB'];
            const i = Math.floor(Math.log(bytes) / Math.log(k));
            return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
        }

        // Model information database (GPU-Accelerated Performance)
        const modelDatabase = {
            'deepseek-coder:6.7b': {
                name: 'DeepSeek-Coder 6.7B',
                description: 'üöÄ GPU-Accelerated coding specialist. 15x faster than CPU! Optimized for Kotlin, TypeScript, Python development.',
                type: 'Coding Specialist',
                vram: '~8GB',
                performance: '~3 seconds (was 47s on CPU)',
                strengths: ['Code Generation', 'Debugging', 'Technical Documentation', 'Architecture']
            },
            'codellama:13b': {
                name: 'Code Llama 13B',
                description: 'ü¶ô Meta\'s specialized coding model with GPU acceleration. Excellent for code completion and large context tasks.',
                type: 'Coding Alternative',
                vram: '~10GB',
                performance: '~6 seconds (was 35s on CPU)',
                strengths: ['Code Completion', 'Multi-language Support', 'Large Context', 'Documentation']
            },
            'dolphin-mixtral:8x7b-v2.7-q4_K_M': {
                name: 'Dolphin-Mixtral 8x7B',
                description: 'üê¨ Uncensored creative powerhouse. Zero refusals, handles controversial topics, perfect for edgy content.',
                type: 'Creative/Uncensored',
                vram: '~14GB',
                performance: '~80 seconds (was 100s on CPU)',
                strengths: ['Creative Writing', 'No Censorship', 'Marketing Copy', 'Roleplay', 'Controversial Topics']
            },
            'dolphin-llama3:8b-v2.9-q5_K_M': {
                name: 'Dolphin-Llama3 8B',
                description: 'üåä Lightweight uncensored model. Fast creative writing without content restrictions or moral policing.',
                type: 'Creative/Lightweight',
                vram: '~6GB',
                performance: '~4 seconds (estimated)',
                strengths: ['Fast Generation', 'Creative Writing', 'No Censorship', 'Efficient', 'Quick Responses']
            },
            'openhermes:latest': {
                name: 'OpenHermes 7B',
                description: 'üìú Legacy general-purpose model. Moderate content filtering, balanced but slower than specialized models.',
                type: 'General Purpose (Legacy)',
                vram: '~4GB',
                performance: '~30 seconds (moderate speed)',
                strengths: ['General Tasks', 'Balanced Performance', 'Stable', 'Moderate Filtering']
            }
        };

        // Handle model selection change
        function handleModelChange() {
            const selectedModel = modelSelect.value;
            currentModel = selectedModel;

            const info = modelDatabase[selectedModel];
            if (info) {
                modelInfoDisplay.innerHTML = `
                    <strong>${info.name}:</strong> ${info.description}<br>
                    <strong>Performance:</strong> ${info.performance} | <strong>VRAM:</strong> ${info.vram} | <strong>Type:</strong> ${info.type}<br>
                    <strong>Strengths:</strong> ${info.strengths.join(', ')}
                `;
            }

            // Show appropriate examples based on model type
            const codingExamples = document.getElementById('codingExamples');
            const creativeExamples = document.getElementById('creativeExamples');

            if (selectedModel.includes('coder') || selectedModel.includes('codellama')) {
                codingExamples.style.display = 'block';
                creativeExamples.style.display = 'none';
            } else if (selectedModel.includes('dolphin')) {
                codingExamples.style.display = 'none';
                creativeExamples.style.display = 'block';
            } else {
                codingExamples.style.display = 'block';
                creativeExamples.style.display = 'block';
            }

            updateButtons();
        }

        // Generate text (complete response)
        async function generateText() {
            if (!serviceHealthy || isGenerating) return;

            const prompt = promptInput.value.trim();
            if (!prompt) return;

            try {
                isGenerating = true;
                updateButtons();
                showResult('Generating response...', false);

                const startTime = Date.now();

                const requestBody = {
                    model: currentModel,
                    prompt: prompt,
                    stream: false,
                    options: {
                        temperature: parseFloat(document.getElementById('temperature').value),
                        num_predict: parseInt(document.getElementById('maxTokens').value),
                        top_p: parseFloat(document.getElementById('topP').value)
                    }
                };

                const response = await fetch('http://localhost:11434/api/generate', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify(requestBody)
                });

                if (!response.ok) {
                    throw new Error(`Generation failed: ${response.status} ${response.statusText}`);
                }

                const data = await response.json();
                const endTime = Date.now();
                const responseTime = endTime - startTime;

                showResult(`Prompt: ${prompt}\n\n--- Response ---\n\n${data.response}`, false);

                // Update stats
                const tokenCount = data.response.split(/\s+/).length; // Rough token estimate
                const tokensPerSecond = (tokenCount / (responseTime / 1000)).toFixed(1);

                updateStats(responseTime, tokenCount, tokensPerSecond);

            } catch (error) {
                showResult(`Generation error: ${error.message}`, true);
                console.error('Generation error:', error);
            } finally {
                isGenerating = false;
                updateButtons();
            }
        }

        // Stream text (real-time response)
        async function streamText() {
            if (!serviceHealthy || isGenerating) return;

            const prompt = promptInput.value.trim();
            if (!prompt) return;

            try {
                isGenerating = true;
                updateButtons();
                showResult(`Prompt: ${prompt}\n\n--- Streaming Response ---\n\n`, false);

                const startTime = Date.now();
                let accumulatedResponse = '';

                const requestBody = {
                    model: currentModel,
                    prompt: prompt,
                    stream: true,
                    options: {
                        temperature: parseFloat(document.getElementById('temperature').value),
                        num_predict: parseInt(document.getElementById('maxTokens').value),
                        top_p: parseFloat(document.getElementById('topP').value)
                    }
                };

                const response = await fetch('http://localhost:11434/api/generate', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify(requestBody)
                });

                if (!response.ok) {
                    throw new Error(`Streaming failed: ${response.status} ${response.statusText}`);
                }

                const reader = response.body.getReader();
                const decoder = new TextDecoder();

                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;

                    const chunk = decoder.decode(value);
                    const lines = chunk.split('\n').filter(line => line.trim());

                    for (const line of lines) {
                        try {
                            const data = JSON.parse(line);
                            if (data.response) {
                                accumulatedResponse += data.response;
                                showResult(`Prompt: ${prompt}\n\n--- Streaming Response ---\n\n${accumulatedResponse}`, false);
                            }

                            if (data.done) {
                                const endTime = Date.now();
                                const responseTime = endTime - startTime;
                                const tokenCount = accumulatedResponse.split(/\s+/).length;
                                const tokensPerSecond = (tokenCount / (responseTime / 1000)).toFixed(1);
                                updateStats(responseTime, tokenCount, tokensPerSecond);
                                break;
                            }
                        } catch (e) {
                            // Skip invalid JSON lines
                            continue;
                        }
                    }
                }

            } catch (error) {
                showResult(`Streaming error: ${error.message}`, true);
                console.error('Streaming error:', error);
            } finally {
                isGenerating = false;
                updateButtons();
            }
        }

        // Show result
        function showResult(text, isError) {
            result.style.display = 'block';
            result.className = `result ${isError ? 'error' : ''}`;
            result.textContent = text;

            // Auto-scroll to bottom for streaming
            result.scrollTop = result.scrollHeight;
        }

        // Update statistics display
        function updateStats(responseTime, tokenCount, tokensPerSecond) {
            document.getElementById('responseTime').textContent = `${responseTime}ms`;
            document.getElementById('tokenCount').textContent = tokenCount;
            document.getElementById('tokensPerSecond').textContent = tokensPerSecond;

            // Show which model was used
            const modelInfo = modelDatabase[currentModel];
            const modelDisplayName = modelInfo ? modelInfo.name : currentModel;
            document.getElementById('modelUsed').textContent = modelDisplayName;

            stats.style.display = 'flex';
        }

        // Event listeners
        promptInput.addEventListener('input', updateButtons);
        promptInput.addEventListener('keydown', (e) => {
            if (e.ctrlKey && e.key === 'Enter') {
                e.preventDefault();
                generateText();
            }
        });
        modelSelect.addEventListener('change', handleModelChange);

        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            checkServiceHealth();
            checkCapabilities();
            handleModelChange(); // Initialize model info display
            updateButtons();

            // Check service health every 30 seconds
            setInterval(checkServiceHealth, 30000);
        });
    </script>
</body>
</html>
