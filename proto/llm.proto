// ============================================================================
// LLM Service Protocol Buffer Definition - DRY Refactored
// ============================================================================
//
// @file llm.proto
// @version 1.1.0
// @author Unhinged Team
// @date 2025-01-04
// @description gRPC service definition for LLM completion and model management
//
// This service provides:
// - LLM completion (synchronous and streaming)
// - Model management and capabilities
// - Token estimation and usage tracking
// - Integration with chat messages and DocumentStore context
//
// DRY Benefits:
// - Uses common.proto for streaming, usage metrics, pagination
// - Reuses chat.proto message types for consistency
// - Standardized error handling and health checks
// ============================================================================

syntax = "proto3";

package unhinged.llm.v1;

import "common.proto";
import "chat.proto";

option java_package = "unhinged.llm";
option java_multiple_files = true;
option java_outer_classname = "LLMProto";

// ============================================================================
// LLM Service Definition
// ============================================================================

/**
 * LLM service for completion and model management
 * 
 * Integrates with chat service for conversation context
 * and uses common patterns for consistency
 */
service LLMService {
  // Completion operations
  rpc GenerateCompletion(CompletionRequest) returns (CompletionResponse);
  rpc StreamCompletion(CompletionRequest) returns (stream common.v1.StreamChunk);  // ← DRY!
  
  // Model management
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse);
  rpc GetModel(GetModelRequest) returns (GetModelResponse);
  
  // Token operations
  rpc EstimateTokens(TokenEstimationRequest) returns (TokenEstimationResponse);
  rpc CountTokens(TokenCountRequest) returns (TokenCountResponse);
  
  // Standard health check
  rpc HealthCheck(common.v1.HealthCheckRequest) returns (common.v1.HealthCheckResponse);  // ← DRY!
}

// ============================================================================
// Core Message Types
// ============================================================================

/**
 * LLM completion request with context integration
 */
message CompletionRequest {
  string model = 1;
  repeated chat.v1.ChatMessage messages = 2;  // ← DRY! (reuse chat messages)
  CompletionOptions options = 3;
  
  string session_id = 4;            // For DocumentStore context integration
  bool include_context = 5;         // Pull relevant documents from DocumentStore
  ContextOptions context_options = 6;
}

/**
 * Completion configuration options
 */
message CompletionOptions {
  int32 max_tokens = 1;
  float temperature = 2;
  float top_p = 3;
  float frequency_penalty = 4;
  float presence_penalty = 5;
  repeated string stop_sequences = 6;
  
  bool stream = 7;                  // Enable streaming response
  bool include_usage = 8;           // Include token usage in response
  int32 n = 9;                      // Number of completions to generate
  
  // Tool calling
  bool enable_tools = 10;
  repeated ToolDefinition tools = 11;
  string tool_choice = 12;          // "auto", "none", or specific tool name
}

/**
 * Context integration options for DocumentStore
 */
message ContextOptions {
  int32 max_context_tokens = 1;     // Maximum tokens to use for context
  float relevance_threshold = 2;    // Minimum relevance score for documents
  repeated string document_types = 3; // Filter by document types
  bool include_metadata = 4;        // Include document metadata in context
}

/**
 * Tool definition for function calling
 */
message ToolDefinition {
  string name = 1;
  string description = 2;
  string parameters_schema = 3;     // JSON schema for parameters
  bool required = 4;
}

/**
 * LLM completion response
 */
message CompletionResponse {
  common.v1.StandardResponse response = 1;  // ← DRY! (success, error handling)
  
  string completion_id = 2;
  repeated CompletionChoice choices = 3;
  common.v1.UsageMetrics usage = 4;         // ← DRY! (token usage)
  CompletionMetadata metadata = 5;
}

/**
 * Individual completion choice
 */
message CompletionChoice {
  int32 index = 1;
  chat.v1.ChatMessage message = 2;          // ← DRY! (reuse chat message)
  string finish_reason = 3;        // "stop", "length", "tool_calls", etc.
  float logprobs = 4;               // Log probability (if requested)
}

/**
 * Completion metadata and diagnostics
 */
message CompletionMetadata {
  string model = 1;
  string provider = 2;              // "openai", "anthropic", "local", etc.
  float processing_time_ms = 3;
  int32 context_documents_used = 4; // Number of DocumentStore docs included
  repeated string context_document_ids = 5;
}

/**
 * LLM model information
 */
message Model {
  common.v1.ResourceMetadata metadata = 1;  // ← DRY! (id, timestamps, etc.)
  
  string name = 2;
  string provider = 3;
  string description = 4;
  
  ModelCapabilities capabilities = 5;
  ModelPricing pricing = 6;
  ModelLimits limits = 7;
  
  bool is_available = 8;
  string status = 9;                // "active", "deprecated", "beta"
}

/**
 * Model capabilities and features
 */
message ModelCapabilities {
  bool supports_streaming = 1;
  bool supports_vision = 2;
  bool supports_function_calling = 3;
  bool supports_json_mode = 4;
  bool supports_system_messages = 5;
  
  repeated string supported_languages = 6;
  repeated string input_modalities = 7;    // "text", "image", "audio"
  repeated string output_modalities = 8;
}

/**
 * Model pricing information
 */
message ModelPricing {
  float input_cost_per_1k_tokens = 1;
  float output_cost_per_1k_tokens = 2;
  string currency = 3;              // "USD"
  bool has_free_tier = 4;
  int32 free_tier_limit = 5;        // Tokens per month
}

/**
 * Model limits and constraints
 */
message ModelLimits {
  int32 max_context_tokens = 1;
  int32 max_output_tokens = 2;
  int32 max_requests_per_minute = 3;
  int32 max_tokens_per_minute = 4;
}

// ============================================================================
// Streaming Payload Types
// ============================================================================

/**
 * Streaming completion chunk payload
 * Used with common.v1.StreamChunk.structured field
 */
message CompletionChunkPayload {
  string completion_id = 1;
  int32 choice_index = 2;
  CompletionChunkType type = 3;
  string content = 4;
  repeated chat.v1.ToolCall tool_calls = 5;  // ← DRY! (reuse from chat)
  string finish_reason = 6;
  common.v1.UsageMetrics usage = 7;          // ← DRY! (only in final chunk)
}

/**
 * Streaming chunk types
 */
enum CompletionChunkType {
  COMPLETION_CHUNK_TYPE_UNSPECIFIED = 0;
  COMPLETION_CHUNK_TYPE_CONTENT = 1;        // Text content
  COMPLETION_CHUNK_TYPE_TOOL_CALL = 2;      // Function call
  COMPLETION_CHUNK_TYPE_TOOL_RESULT = 3;    // Function result
  COMPLETION_CHUNK_TYPE_THINKING = 4;       // Model reasoning (if supported)
  COMPLETION_CHUNK_TYPE_METADATA = 5;       // Additional metadata
}

// ============================================================================
// Request/Response Messages
// ============================================================================

message ListModelsRequest {
  string provider = 1;              // Filter by provider
  bool available_only = 2;          // Only show available models
  common.v1.PaginationRequest pagination = 3;  // ← DRY!
  repeated common.v1.Filter filters = 4;       // ← DRY!
}

message ListModelsResponse {
  common.v1.StandardResponse response = 1;     // ← DRY!
  repeated Model models = 2;
  common.v1.PaginationResponse pagination = 3; // ← DRY!
}

message GetModelRequest {
  string model_id = 1;
  bool include_pricing = 2;
  bool include_capabilities = 3;
}

message GetModelResponse {
  common.v1.StandardResponse response = 1;  // ← DRY!
  Model model = 2;
}

message TokenEstimationRequest {
  string model = 1;
  oneof input {
    string text = 2;
    repeated chat.v1.ChatMessage messages = 3;  // ← DRY!
  }
  bool include_context = 4;         // Estimate with DocumentStore context
  string session_id = 5;            // For context estimation
}

message TokenEstimationResponse {
  common.v1.StandardResponse response = 1;  // ← DRY!
  common.v1.TokenUsage usage = 2;           // ← DRY!
  TokenBreakdown breakdown = 3;
}

/**
 * Detailed token count breakdown
 */
message TokenBreakdown {
  int32 system_tokens = 1;
  int32 user_tokens = 2;
  int32 assistant_tokens = 3;
  int32 context_tokens = 4;         // From DocumentStore
  int32 tool_tokens = 5;            // Function definitions
  
  repeated DocumentTokens context_documents = 6;
}

/**
 * Token count per context document
 */
message DocumentTokens {
  string document_id = 1;
  string document_type = 2;
  int32 token_count = 3;
  float relevance_score = 4;
}

message TokenCountRequest {
  string model = 1;
  string text = 2;
}

message TokenCountResponse {
  common.v1.StandardResponse response = 1;  // ← DRY!
  int32 token_count = 2;
  repeated TokenInfo tokens = 3;    // Detailed token information (optional)
}

/**
 * Individual token information for debugging
 */
message TokenInfo {
  string token = 1;
  int32 token_id = 2;
  float probability = 3;
}
